# -*- coding: utf-8 -*-
"""notebookRam25gb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YROWENV1Ko4XWTKZeYIlQS8RuQPwEzVs
"""

# Increase the RAM capacity of colab
from google.colab import drive
drive.flush_and_unmount()

# Install required packages
!pip install pyspark -U sentence-transformers
!pip install --upgrade --force-reinstall git+https://github.com/facebookresearch/GENRE.git

# Import relevant packages
from genre.hf_model import GENRE
from sentence_transformers import SentenceTransformer
from datetime import datetime
from genre.entity_linking import get_end_to_end_prefix_allowed_tokens_fn_hf as get_prefix_allowed_tokens_fn
from genre.utils import get_entity_spans_hf as get_entity_spans
import os
import sys
import re
import numpy as np
from timeit import default_timer as timer
import json
from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, FloatType
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from sentence_transformers import SentenceTransformer
import time
import ast
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LinearSVC, LogisticRegression, RandomForestClassifier
from pyspark.ml.regression import LinearRegression
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

# Mount colab with google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Specify relevant paths
folder_path_train = '/content/drive/MyDrive/DMT/part2_res/'
train_df = '/content/drive/MyDrive/DMT/Final_results_part2/train_set.jsonl'
folder_path_dev = '/content/drive/MyDrive/DMT/dev_res/'
dev_df = '/content/drive/MyDrive/DMT/Final_results_part2/dev_set.jsonl'
folder_path_test = '/content/drive/MyDrive/DMT/test_res/'
test_df = '/content/drive/MyDrive/DMT/Final_results_part2/test_set.jsonl'

# Biuld PySpark environment
spark = SparkSession.builder \
    .master('local[*]') \
    .config("spark.driver.memory", "15g") \
    .appName('Homework3') \
    .getOrCreate()

# Read dataframes and convert into pyspark objects
df_train = spark.read.json(train_df)
df_dev = spark.read.json(dev_df)
df_test = spark.read.json(test_df)

# Function to process data. At this stage the only thing we have to do is:
# - Merge Claim and Abstract Embeddings
# - Convert label into integer (0/1)
def data_processing(data):

    # Vectorize the claim_embedding
    to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())
    data = data.withColumn('claim_embedding', to_vector(col("claim_embedding")))
    data = data.withColumn("abstract_embedding", to_vector(col("abstract_embedding")))
    
    assemble = VectorAssembler(inputCols=["claim_embedding", "abstract_embedding"], outputCol="features")
    data = assemble.transform(data)

    if 'output' in data.columns:
        # Label our dataset
        label_udf = udf(lambda output: 1 if ast.literal_eval(output)[0].get('anwser') == "SUPPORTS" else 0, IntegerType())
        data = data.withColumn('label', label_udf(col('output')))
    
    return data

# Process data
df_train = data_processing(df_train)
df_dev = data_processing(df_dev)
df_test = data_processing(df_test)

# Make sure the training dataset is balanced (it should be because we have balanced the training set in the previous section
df_train.groupBy('label').count().show()

## Logistic Regression (Classifier 1)
# Call the Logistic Regression Model
lr = LogisticRegression(labelCol='label', featuresCol='features')

# Set up the relevant Grid Search Parameters
paramGrid = (ParamGridBuilder() 
                .addGrid(lr.regParam, [0.01, 0.5]) 
                .addGrid(lr.elasticNetParam, [0,0.5,1.0]) 
                .addGrid(lr.maxIter, [1,3,5,10,100]) 
                .build())


# Provide class which will be used for the evaluation
evaluator = BinaryClassificationEvaluator()

# Define the Cross Validator
cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator = evaluator, numFolds=3 )

# Fit the model
%%time
cvModel = cv.fit(df_train)

# Since this is a time consuming task, for security we will save the model accordingly
cvModel.save('/content/drive/MyDrive/DMT/lr_model_part2)

# Used to read the saved model
from pyspark.ml.tuning import CrossValidatorModel
persistedModel = CrossValidatorModel.load('/content/drive/MyDrive/DMT/lr_model_part2')

# Get the best model parameters
best_model = persistedModel.bestModel

print('Best Param (regParam):', best_model._java_obj.getRegParam())
print('MaxIter:', best_model._java_obj.getMaxIter())
print('elasticNetParam', best_model._java_obj.getElasticNetParam())

# Function used to evaluate the best model over a specific data set
def evaluate_model(model, dev_data):

    ## Evaluate Best Model
    predictions = model.transform(dev_data)

    preds_and_labels = predictions.select(['prediction','label'])

    metrics = MulticlassMetrics(preds_and_labels.rdd.map(lambda x: tuple(map(float, x))))

    cm=metrics.confusionMatrix().toArray()

    print('confustion Matrix', cm)

    print("Accuracy", (cm[0][0]+cm[1][1])/cm.sum())

    print('Precision of REFUTES:', metrics.precision(0))
    print('Precision of SUPPORT:', metrics.precision(1))

    print('Recall of REFUTES:', metrics.recall(0))
    print('Recall of SUPPORT:', metrics.recall(1))

# Evaluate the model over the development sample
evaluate_model(best_model, df_dev)




## Linear Support Vector Machine (Classifier 2)
# Call the Model
from pyspark.ml.classification import LinearSVC
lsvc = LinearSVC(labelCol='label', featuresCol='features')

# Set up the relevant Grid Search Parameters
paramGrid = (ParamGridBuilder() 
                .addGrid(lscv.regParam, [0.01,0.5]) 
                .addGrid(lscv.maxIter, [1,3,5,10,100]) 
                .build())

# Provide class which will be used for the evaluation
evaluator = BinaryClassificationEvaluator()

# Define the Cross Validator
cv_lsvc = CrossValidator(estimator=lsvc, estimatorParamMaps=paramGrid, evaluator = evaluator, numFolds=5 )

# Fit the model
%%time
lsvc_Model = cv_lsvc.fit(df_train)

# Save model
lsvc_Model.save('/content/drive/MyDrive/DMT/lsvc_model_part2_best_100_full_5')

# Read model
from pyspark.ml.tuning import CrossValidatorModel
persistedModel = CrossValidatorModel.load('/content/drive/MyDrive/DMT/lsvc_model_part2_best_200_full')

# Best Model Parameters
best_model = lsvc_Model.bestModel
print('Best Param (regParam):', best_model._java_obj.getRegParam())
print('MaxIter:', best_model._java_obj.getMaxIter())

# Evaluate Best Model
evaluate_model(lsvc_Model, df_dev)