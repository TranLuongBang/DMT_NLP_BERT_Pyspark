# -*- coding: utf-8 -*-
"""SubmissionKILT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xnIwZ7-YztMpDjW8qBtrxAifYpvzRxsR
"""

# Install necessary packages
!pip install pyspark -U sentence-transformers
!pip install --upgrade --force-reinstall git+https://github.com/facebookresearch/GENRE.git

# Import packages
from genre.hf_model import GENRE
from sentence_transformers import SentenceTransformer
from datetime import datetime
from genre.entity_linking import get_end_to_end_prefix_allowed_tokens_fn_hf as get_prefix_allowed_tokens_fn
from genre.utils import get_entity_spans_hf as get_entity_spans
import os
import sys
import re
import numpy as np
from timeit import default_timer as timer
import json
from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, FloatType
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from sentence_transformers import SentenceTransformer
import time
import ast
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LinearSVC, LogisticRegression, RandomForestClassifier
from pyspark.ml.regression import LinearRegression
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics
import json

# Mount colab on google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Start spark session
spark = SparkSession.builder \
    .master('local[*]') \
    .config("spark.driver.memory", "15g") \
    .appName('Homework3') \
    .getOrCreate()

# Function used to evaluate the best model over a specific data set
def evaluate_model(model, dev_data):

    ## Evaluate Best Model
    predictions = model.transform(dev_data)

    preds_and_labels = predictions.select(['prediction','label'])
    return preds_and_labels


# Function to dump jsonl from nested list
def dump_jsonl(data, output_path, append=False):
    """
    Write list of objects to a JSON lines file.
    """
    mode = 'a+' if append else 'w'
    with open(output_path, mode, encoding='utf-8') as f:
        for line in data:
            json_record = json.dumps(line, ensure_ascii=False)
            f.write(json_record + '\n')


# Load models
from pyspark.ml.tuning import CrossValidatorModel
persistedModel_lr = CrossValidatorModel.load('/content/drive/MyDrive/DMT/lr_model_part2')
persistedModel_lscv = CrossValidatorModel.load('/content/drive/MyDrive/DMT/lsvc_model_part2_best_100_full')


best_model_lr = lr_Model.bestModel
best_model_lscv = lsvc_Model.bestModel


data = []
with open('/content/drive/MyDrive/DMT/test_kilt.json', 'r', encoding='utf-8') as json_file:
    for line in json_file:
        temp = json.loads(line)
        temp_save = {}
        temp_save['id'] = temp['id']
        temp_save['output'] = [{'answer': 'SUPPORTS' if temp['prediction'] == 1.0 else 'REFUTES'}]
        data.append(temp_save)


dump_jsonl(data=data, output_path='/content/drive/MyDrive/DMT/test_set_result.jsonl', append=False)

